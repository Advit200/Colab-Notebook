{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-08-07T12:26:41.839576Z",
     "iopub.status.busy": "2021-08-07T12:26:41.839119Z",
     "iopub.status.idle": "2021-08-07T12:26:41.867700Z",
     "shell.execute_reply": "2021-08-07T12:26:41.865960Z",
     "shell.execute_reply.started": "2021-08-07T12:26:41.839470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/finaldata/Final_PowerBI_input.xlsx\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT Notes :**\n",
    "\n",
    "1. BERT is transformer based model which is different from LLSTM model in the sense,text doesnt have to put in a sequence in BERT compared to LSTM models.So it is fast as a whole sentence can be splitted into words and put at once i.e parallel processing of the words are possible.Even though the words are put at once,BERT has the ability to catch the sequence btw the words and the context in which it is being used in the sentence.This also solves one of the problem with the LSTM models where when the sentences become very long,there are chances of the LSTM model to loose the context and effect of one word on other words.This is solved in BERT model by assigning position to each of the words of the sentence before inserting into the attention layers and is achieved by the \"positional encoding\" layer.\n",
    "\n",
    "2. BERT is also different from other DL models in the sense that it generates \"contextual embeddings\" i.e a word has different vector representation depending on the contect in which is used.This is lacked by the word2vec or glove models.\n",
    "\n",
    "3. BERT is basically trained for two tasks : First for masked language modelling ( masking 15 % of the words in the corpus and then asking the NN to preddict those words ) and second for next sentence prediction ( given two sentences,predicte that if the next sentence is continuation/follow of the previous sentence.\n",
    "\n",
    "4. BERT can be very easily fine tuned for the specifics NLP tasks like sentiment analysis,question answering etc.It has million of parameters,so even fine tunning the BERT model for 1-2 epochs enchanes the performace a lot.\n",
    "\n",
    "5. BERT has a non-directional artitecture compared to bidirectional or single in LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T13:24:43.940008Z",
     "iopub.status.busy": "2021-08-07T13:24:43.939664Z",
     "iopub.status.idle": "2021-08-07T13:24:43.945033Z",
     "shell.execute_reply": "2021-08-07T13:24:43.944044Z",
     "shell.execute_reply.started": "2021-08-07T13:24:43.939977Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn,optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score,matthews_corrcoef,confusion_matrix,classification_report\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:26:44.129138Z",
     "iopub.status.busy": "2021-08-07T12:26:44.128817Z",
     "iopub.status.idle": "2021-08-07T12:26:44.193051Z",
     "shell.execute_reply": "2021-08-07T12:26:44.191951Z",
     "shell.execute_reply.started": "2021-08-07T12:26:44.129110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:26:44.195622Z",
     "iopub.status.busy": "2021-08-07T12:26:44.195170Z",
     "iopub.status.idle": "2021-08-07T12:26:54.254598Z",
     "shell.execute_reply": "2021-08-07T12:26:54.253604Z",
     "shell.execute_reply.started": "2021-08-07T12:26:44.195581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.0.7-py2.py3-none-any.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 624 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nlpaug\n",
      "  Downloading nlpaug-1.1.7-py3-none-any.whl (405 kB)\n",
      "\u001b[K     |████████████████████████████████| 405 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl, nlpaug\n",
      "Successfully installed et-xmlfile-1.1.0 nlpaug-1.1.7 openpyxl-3.0.7\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install openpyxl nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:26:54.256569Z",
     "iopub.status.busy": "2021-08-07T12:26:54.256220Z",
     "iopub.status.idle": "2021-08-07T12:27:03.123897Z",
     "shell.execute_reply": "2021-08-07T12:27:03.122025Z",
     "shell.execute_reply.started": "2021-08-07T12:26:54.256531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Pros</th>\n",
       "      <th>Cons</th>\n",
       "      <th>Main_Review</th>\n",
       "      <th>Review_id</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1. This company has eight hours shift round th...</td>\n",
       "      <td>1</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with all jobs, your experience will depend ...</td>\n",
       "      <td>2</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Good benefits</td>\n",
       "      <td>Work your life away</td>\n",
       "      <td>Very hot and dirty working conditions very old...</td>\n",
       "      <td>3</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>Full benefits, good pay, mist fans</td>\n",
       "      <td>The Texas heat</td>\n",
       "      <td>Everyone is pretty helpful there especially be...</td>\n",
       "      <td>4</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>Benefits</td>\n",
       "      <td>Oil related</td>\n",
       "      <td>Rewarding place to work. Everyone is professio...</td>\n",
       "      <td>5</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21561</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10884</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21562</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10885</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21563</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10886</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21564</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10887</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21565</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10888</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10888 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Rating                                Pros                 Cons  \\\n",
       "0           4                                 NaN                  NaN   \n",
       "2           4                                 NaN                  NaN   \n",
       "5           5                       Good benefits  Work your life away   \n",
       "7           5  Full benefits, good pay, mist fans       The Texas heat   \n",
       "10          5                            Benefits          Oil related   \n",
       "...       ...                                 ...                  ...   \n",
       "21561       4                                 NaN                  NaN   \n",
       "21562       1                                 NaN                  NaN   \n",
       "21563       4                                 NaN                  NaN   \n",
       "21564       3                                 NaN                  NaN   \n",
       "21565       3                                 NaN                  NaN   \n",
       "\n",
       "                                             Main_Review  Review_id Sentiment  \n",
       "0      1. This company has eight hours shift round th...          1  POSITIVE  \n",
       "2      As with all jobs, your experience will depend ...          2  POSITIVE  \n",
       "5      Very hot and dirty working conditions very old...          3   NEUTRAL  \n",
       "7      Everyone is pretty helpful there especially be...          4  POSITIVE  \n",
       "10     Rewarding place to work. Everyone is professio...          5  POSITIVE  \n",
       "...                                                  ...        ...       ...  \n",
       "21561                                                NaN      10884  POSITIVE  \n",
       "21562                                                NaN      10885  NEGATIVE  \n",
       "21563                                                NaN      10886  POSITIVE  \n",
       "21564                                                NaN      10887  POSITIVE  \n",
       "21565                                                NaN      10888  POSITIVE  \n",
       "\n",
       "[10888 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"../input/finaldata/Final_PowerBI_input.xlsx\",usecols=[\"Rating\",\"Main_Review\",\"Pros\",\"Cons\",\"Sentiment\",\"Review_id\"],engine=\"openpyxl\")\n",
    "df.drop_duplicates(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:03.125651Z",
     "iopub.status.busy": "2021-08-07T12:27:03.125306Z",
     "iopub.status.idle": "2021-08-07T12:27:03.310452Z",
     "shell.execute_reply": "2021-08-07T12:27:03.309523Z",
     "shell.execute_reply.started": "2021-08-07T12:27:03.125615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.Rating.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**{0:\"NEGATIVE\",\n",
    "   1:\"NEUTRAL\",\n",
    "   2:\"POSITIVE\"}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:03.312247Z",
     "iopub.status.busy": "2021-08-07T12:27:03.311881Z",
     "iopub.status.idle": "2021-08-07T12:27:03.812696Z",
     "shell.execute_reply": "2021-08-07T12:27:03.811884Z",
     "shell.execute_reply.started": "2021-08-07T12:27:03.312209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Review</th>\n",
       "      <th>Actual_Sentiment</th>\n",
       "      <th>Review_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1. This company has eight hours shift round th...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>As with all jobs, your experience will depend ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Very hot and dirty working conditions very old...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Everyone is pretty helpful there especially be...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Rewarding place to work. Everyone is professio...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21688</th>\n",
       "      <td>21688</td>\n",
       "      <td>Not much of work at the start</td>\n",
       "      <td>0</td>\n",
       "      <td>10858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21689</th>\n",
       "      <td>21689</td>\n",
       "      <td>its a captive company but work wise worst then...</td>\n",
       "      <td>0</td>\n",
       "      <td>10859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21690</th>\n",
       "      <td>21690</td>\n",
       "      <td>None whatsoever</td>\n",
       "      <td>0</td>\n",
       "      <td>10860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21691</th>\n",
       "      <td>21691</td>\n",
       "      <td>Poor management</td>\n",
       "      <td>0</td>\n",
       "      <td>10862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21692</th>\n",
       "      <td>21692</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>10863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21693 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                             Review  \\\n",
       "0          0  1. This company has eight hours shift round th...   \n",
       "1          1  As with all jobs, your experience will depend ...   \n",
       "2          2  Very hot and dirty working conditions very old...   \n",
       "3          3  Everyone is pretty helpful there especially be...   \n",
       "4          4  Rewarding place to work. Everyone is professio...   \n",
       "...      ...                                                ...   \n",
       "21688  21688                      Not much of work at the start   \n",
       "21689  21689  its a captive company but work wise worst then...   \n",
       "21690  21690                                    None whatsoever   \n",
       "21691  21691                                    Poor management   \n",
       "21692  21692                                                 No   \n",
       "\n",
       "       Actual_Sentiment  Review_id  \n",
       "0                     2          1  \n",
       "1                     2          2  \n",
       "2                     2          3  \n",
       "3                     2          4  \n",
       "4                     2          5  \n",
       "...                 ...        ...  \n",
       "21688                 0      10858  \n",
       "21689                 0      10859  \n",
       "21690                 0      10860  \n",
       "21691                 0      10862  \n",
       "21692                 0      10863  \n",
       "\n",
       "[21693 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For \"Main_Review\" column, assigning review based on \"Rating' column\n",
    "\n",
    "temp_df = df.copy()\n",
    "temp_df[\"Actual_Sentiment\"] = temp_df.loc[:,\"Rating\"].apply(lambda x : 0 if x <= 2 else 1 if x == 3 else 2)\n",
    "temp_df[\"Review\"] = temp_df[\"Main_Review\"].str.replace(r\"'|\\\"|\\\"|'|!|\\s+\", \" \",regex=True)\n",
    "temp_df.dropna(subset=[\"Review\"],inplace=True)\n",
    "\n",
    "temp_df = temp_df[[\"Review\",\"Actual_Sentiment\",\"Review_id\"]]\n",
    "temp_df\n",
    "\n",
    "######################################################################\n",
    "## Assigning POSITIVE sentiment to all the reviews in \"Pros\" category.\n",
    "\n",
    "temp_df_1 = df.copy()\n",
    "temp_df_1[\"Actual_Sentiment\"] = 2\n",
    "temp_df_1[\"Review\"] = temp_df_1[\"Pros\"].str.replace(r\"'|\\\"|\\\"|'|!|\\s+\", \" \",regex=True)\n",
    "temp_df_1.dropna(subset=[\"Review\"],inplace=True)\n",
    "\n",
    "temp_df_pros = temp_df_1[[\"Review\",\"Actual_Sentiment\",\"Review_id\"]]\n",
    "temp_df_pros\n",
    "\n",
    "######################################################################\n",
    "## Assigning NEGATIVE sentiment to all the reviews in \"Cons\" category.\n",
    "\n",
    "temp_df_2 = df.copy()\n",
    "temp_df_2[\"Actual_Sentiment\"] = 0\n",
    "temp_df_2[\"Review\"] = temp_df_2[\"Cons\"].str.replace(r\"'|\\\"|\\\"|'|!|\\s+\", \" \",regex=True)\n",
    "temp_df_2.dropna(subset=[\"Review\"],inplace=True)\n",
    "\n",
    "temp_df_cons = temp_df_2[[\"Review\",\"Actual_Sentiment\",\"Review_id\"]]\n",
    "temp_df_cons\n",
    "\n",
    "######################################################################\n",
    "\n",
    "final_df = pd.concat([temp_df,temp_df_pros,temp_df_cons],axis=0,ignore_index=True)\n",
    "final_df.reset_index(inplace=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:03.815838Z",
     "iopub.status.busy": "2021-08-07T12:27:03.815544Z",
     "iopub.status.idle": "2021-08-07T12:27:04.043718Z",
     "shell.execute_reply": "2021-08-07T12:27:04.042700Z",
     "shell.execute_reply.started": "2021-08-07T12:27:03.815812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_df[\"Review_len\"] = final_df[\"Review\"].apply(lambda x : len(x.split()))\n",
    "final_df[\"Review_len\"].plot(kind=\"hist\",xlim=(0,200))\n",
    "\n",
    "# This shows that most of the reviews have less than 100 words in it ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:04.046116Z",
     "iopub.status.busy": "2021-08-07T12:27:04.045763Z",
     "iopub.status.idle": "2021-08-07T12:27:04.059638Z",
     "shell.execute_reply": "2021-08-07T12:27:04.058543Z",
     "shell.execute_reply.started": "2021-08-07T12:27:04.046081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20808, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping data that has only one word in review\n",
    "\n",
    "drop_index = final_df.query('Review_len <=1').index\n",
    "\n",
    "final_df = final_df.loc[~final_df.index.isin(drop_index) , :]\n",
    "\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:04.061579Z",
     "iopub.status.busy": "2021-08-07T12:27:04.061038Z",
     "iopub.status.idle": "2021-08-07T12:27:04.181660Z",
     "shell.execute_reply": "2021-08-07T12:27:04.180717Z",
     "shell.execute_reply.started": "2021-08-07T12:27:04.061543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    10647\n",
      "0     8714\n",
      "1     1447\n",
      "Name: Actual_Sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_df[\"Actual_Sentiment\"].value_counts().plot(kind=\"bar\")\n",
    "\n",
    "print(final_df[\"Actual_Sentiment\"].value_counts())\n",
    "\n",
    "# Neutral sentiment is very less, so need to perform text augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:04.183359Z",
     "iopub.status.busy": "2021-08-07T12:27:04.182977Z",
     "iopub.status.idle": "2021-08-07T12:27:04.196930Z",
     "shell.execute_reply": "2021-08-07T12:27:04.196068Z",
     "shell.execute_reply.started": "2021-08-07T12:27:04.183323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Review</th>\n",
       "      <th>Actual_Sentiment</th>\n",
       "      <th>Review_id</th>\n",
       "      <th>Review_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Everyone is pretty helpful there especially be...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6298</th>\n",
       "      <td>6298</td>\n",
       "      <td>Full benefits, good pay, mist fans</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14020</th>\n",
       "      <td>14020</td>\n",
       "      <td>The Texas heat</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                             Review  \\\n",
       "3          3  Everyone is pretty helpful there especially be...   \n",
       "6298    6298                 Full benefits, good pay, mist fans   \n",
       "14020  14020                                     The Texas heat   \n",
       "\n",
       "       Actual_Sentiment  Review_id  Review_len  \n",
       "3                     2          4          28  \n",
       "6298                  2          4           6  \n",
       "14020                 0          4           3  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.loc[final_df[\"Review_id\"]==4,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting the Neutral Reviews as their count is less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:04.198637Z",
     "iopub.status.busy": "2021-08-07T12:27:04.198142Z",
     "iopub.status.idle": "2021-08-07T12:27:04.370060Z",
     "shell.execute_reply": "2021-08-07T12:27:04.369128Z",
     "shell.execute_reply.started": "2021-08-07T12:27:04.198602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_df.query(\"Actual_Sentiment == 1\")[\"Review_len\"].plot(kind=\"hist\",xlim=(0,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:04.371729Z",
     "iopub.status.busy": "2021-08-07T12:27:04.371385Z",
     "iopub.status.idle": "2021-08-07T12:27:12.939999Z",
     "shell.execute_reply": "2021-08-07T12:27:12.939101Z",
     "shell.execute_reply.started": "2021-08-07T12:27:04.371694Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As with all job, your experience testament depend greatly upon your manager. Ace had great, average and poor coach during my time there. I enjoyed the work and my coworkers. Their benefits bundle and employee perks be great.\n",
      "\n",
      "\n",
      "As with all jobs, your experience will depend greatly upon your handler. 1 had great, fair and poor managers during my time there. I enjoyed the work and my coworkers. Their benefits package and employee perk are keen.\n",
      "\n",
      "\n",
      "As with all jobs, your experience will depend greatly upon your manager. Ace had great, reasonable and poor managers during my time there. I enjoyed the work and my coworkers. Their benefits package and employee fringe benefit personify bang up.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "aug = naw.SynonymAug(aug_src=\"wordnet\",aug_min=10) # Replace the words with their synonym.aug_min tells the no.of words to replace.\n",
    "\n",
    "for i in aug.augment(final_df[\"Review\"][1] , 3): # 3 represents the no,of augmented sentences we want to create.\n",
    "    print(f\"\\n{i}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:12.941803Z",
     "iopub.status.busy": "2021-08-07T12:27:12.941444Z",
     "iopub.status.idle": "2021-08-07T12:27:37.572914Z",
     "shell.execute_reply": "2021-08-07T12:27:37.571806Z",
     "shell.execute_reply.started": "2021-08-07T12:27:12.941767Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Review</th>\n",
       "      <th>Actual_Sentiment</th>\n",
       "      <th>Review_id</th>\n",
       "      <th>Review_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3094</th>\n",
       "      <td>3094</td>\n",
       "      <td>fast pace private enterprise place to work. Fa...</td>\n",
       "      <td>1</td>\n",
       "      <td>3095</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488</th>\n",
       "      <td>2488</td>\n",
       "      <td>Train on motorcar vary, depends mostly on the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2489</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>1534</td>\n",
       "      <td>I learned everything on my ain. I really like ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1535</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>508</td>\n",
       "      <td>Information technology a secure work place if ...</td>\n",
       "      <td>1</td>\n",
       "      <td>509</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3983</th>\n",
       "      <td>3983</td>\n",
       "      <td>Domestic dog eat detent earthly concern. No ad...</td>\n",
       "      <td>1</td>\n",
       "      <td>3984</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                             Review  \\\n",
       "3094   3094  fast pace private enterprise place to work. Fa...   \n",
       "2488   2488  Train on motorcar vary, depends mostly on the ...   \n",
       "1534   1534  I learned everything on my ain. I really like ...   \n",
       "508     508  Information technology a secure work place if ...   \n",
       "3983   3983  Domestic dog eat detent earthly concern. No ad...   \n",
       "\n",
       "      Actual_Sentiment  Review_id  Review_len  \n",
       "3094                 1       3095          28  \n",
       "2488                 1       2489          35  \n",
       "1534                 1       1535          45  \n",
       "508                  1        509          33  \n",
       "3983                 1       3984          11  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Augment_Review(df,augmenter=None,n_sentence=None,Sentiment=None):\n",
    "    temp_df = df.query(f\"Actual_Sentiment == {Sentiment}\")\n",
    "    temp_df.loc[:,\"Review\"] = temp_df.loc[:,\"Review\"].apply(lambda text : [result for result in augmenter.augment(text , n_sentence)])\n",
    "    temp_df = temp_df.explode(\"Review\")\n",
    "    return temp_df\n",
    "    \n",
    "augmented_df = Augment_Review(final_df,augmenter=aug,n_sentence=5,Sentiment=1)\n",
    "augmented_df = shuffle(augmented_df)\n",
    "augmented_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:37.575058Z",
     "iopub.status.busy": "2021-08-07T12:27:37.574555Z",
     "iopub.status.idle": "2021-08-07T12:27:37.600661Z",
     "shell.execute_reply": "2021-08-07T12:27:37.599715Z",
     "shell.execute_reply.started": "2021-08-07T12:27:37.575000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Review</th>\n",
       "      <th>Actual_Sentiment</th>\n",
       "      <th>Review_id</th>\n",
       "      <th>Review_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1. This company has eight hours shift round th...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>As with all jobs, your experience will depend ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Very hot and dirty working conditions very old...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Everyone is pretty helpful there especially be...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Rewarding place to work. Everyone is professio...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26591</th>\n",
       "      <td>5451</td>\n",
       "      <td>Company used to accept a groovy environs with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5452</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26592</th>\n",
       "      <td>2300</td>\n",
       "      <td>I enjoyed the people I worked with very much. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2301</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26593</th>\n",
       "      <td>2224</td>\n",
       "      <td>jolly physically demanding work, just non so m...</td>\n",
       "      <td>1</td>\n",
       "      <td>2225</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26594</th>\n",
       "      <td>4601</td>\n",
       "      <td>Piece of work - life balance, Work - life bala...</td>\n",
       "      <td>1</td>\n",
       "      <td>4602</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26595</th>\n",
       "      <td>2006</td>\n",
       "      <td>A lot of practiced experience from drive heavy...</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26596 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                             Review  \\\n",
       "0          0  1. This company has eight hours shift round th...   \n",
       "1          1  As with all jobs, your experience will depend ...   \n",
       "2          2  Very hot and dirty working conditions very old...   \n",
       "3          3  Everyone is pretty helpful there especially be...   \n",
       "4          4  Rewarding place to work. Everyone is professio...   \n",
       "...      ...                                                ...   \n",
       "26591   5451  Company used to accept a groovy environs with ...   \n",
       "26592   2300  I enjoyed the people I worked with very much. ...   \n",
       "26593   2224  jolly physically demanding work, just non so m...   \n",
       "26594   4601  Piece of work - life balance, Work - life bala...   \n",
       "26595   2006  A lot of practiced experience from drive heavy...   \n",
       "\n",
       "       Actual_Sentiment  Review_id  Review_len  \n",
       "0                     2          1          58  \n",
       "1                     2          2          38  \n",
       "2                     2          3          30  \n",
       "3                     2          4          28  \n",
       "4                     2          5          26  \n",
       "...                 ...        ...         ...  \n",
       "26591                 1       5452          43  \n",
       "26592                 1       2301          36  \n",
       "26593                 1       2225          32  \n",
       "26594                 1       4602         178  \n",
       "26595                 1       2007          22  \n",
       "\n",
       "[26596 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = final_df.query('Actual_Sentiment == 0 or Actual_Sentiment == 2')\n",
    "final_df = pd.concat([final_df,augmented_df] , axis=0,ignore_index=True)\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:37.602302Z",
     "iopub.status.busy": "2021-08-07T12:27:37.601937Z",
     "iopub.status.idle": "2021-08-07T12:27:37.735686Z",
     "shell.execute_reply": "2021-08-07T12:27:37.734734Z",
     "shell.execute_reply.started": "2021-08-07T12:27:37.602245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    0.400323\n",
      "0    0.327643\n",
      "1    0.272033\n",
      "Name: Actual_Sentiment, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_df[\"Actual_Sentiment\"].value_counts().plot(kind=\"bar\")\n",
    "print(final_df[\"Actual_Sentiment\"].value_counts(normalize=True))\n",
    "\n",
    "# Now the dataset looks balanced to some extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:37.737509Z",
     "iopub.status.busy": "2021-08-07T12:27:37.737083Z",
     "iopub.status.idle": "2021-08-07T12:27:41.700456Z",
     "shell.execute_reply": "2021-08-07T12:27:41.699674Z",
     "shell.execute_reply.started": "2021-08-07T12:27:37.737468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23946fb24f76450e9fc9a51387179ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b514561f0ac245819ae79d44cf6b6b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afacf995f5d4826adaaf40cc6b7e5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = \"bert-base-cased\"\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME) # BERT expect the tokens in a specific format,so we are using bert base model for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.702078Z",
     "iopub.status.busy": "2021-08-07T12:27:41.701728Z",
     "iopub.status.idle": "2021-08-07T12:27:41.712706Z",
     "shell.execute_reply": "2021-08-07T12:27:41.711776Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.702041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab()) # So this BERT model has a vocab size of 29k. and it cannnot be changed/appended.If BERT find a word that is not in the vocab,\n",
    "#  it splits that words in the letters of different length and see if those splitted small words are present or not in the vocab .\n",
    "# So every broken word(of the main word in our text) except the first one looks like \"##letter\". If even after splitting the words, BERT doesnt find any \n",
    "# matching token , it will assign it [UNK] token ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.714425Z",
     "iopub.status.busy": "2021-08-07T12:27:41.714043Z",
     "iopub.status.idle": "2021-08-07T12:27:41.723046Z",
     "shell.execute_reply": "2021-08-07T12:27:41.722029Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.714387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEP]\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.sep_token)\n",
    "print(tokenizer.sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.725024Z",
     "iopub.status.busy": "2021-08-07T12:27:41.724670Z",
     "iopub.status.idle": "2021-08-07T12:27:41.734080Z",
     "shell.execute_reply": "2021-08-07T12:27:41.733030Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.724988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As', 'with', 'all', 'jobs', ',', 'your', 'experience', 'will', 'depend', 'greatly', 'upon', 'your', 'manager', '.', 'I', 'had', 'great', ',', 'fair', 'and', 'poor', 'managers', 'during', 'my', 'time', 'there', '.', 'I', 'enjoyed', 'the', 'work', 'and', 'my', 'cow', '##or', '##kers', '.', 'Their', 'benefits', 'package', 'and', 'employee', 'per', '##ks', 'are', 'great', '.']\n",
      "[1249, 1114, 1155, 5448, 117, 1240, 2541, 1209, 12864, 5958, 1852, 1240, 2618, 119, 146, 1125, 1632, 117, 4652, 1105, 2869, 11493, 1219, 1139, 1159, 1175, 119, 146, 4927, 1103, 1250, 1105, 1139, 13991, 1766, 8811, 119, 2397, 6245, 7305, 1105, 7775, 1679, 4616, 1132, 1632, 119]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(final_df[\"Review\"][1])\n",
    "print(tokens)\n",
    "print(tokenizer.convert_tokens_to_ids(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.735899Z",
     "iopub.status.busy": "2021-08-07T12:27:41.735533Z",
     "iopub.status.idle": "2021-08-07T12:27:41.747561Z",
     "shell.execute_reply": "2021-08-07T12:27:41.746628Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.735851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'Z', '##ink', 'Co', '##mb', '##ust', '##ion', 'is', 'one', 'of', 'the', 'greatest', 'companies', 'I', 'have', 'ever', 'worked', 'for', '.', 'Great', 'management', ',', 'training', ',', 'and', 'great', 'opportunities', 'for', 'advancement', '.', 'Would', 'highly', 'recommend', '.']\n",
      "[1287, 163, 10223, 3291, 12913, 8954, 1988, 1110, 1141, 1104, 1103, 4459, 2557, 146, 1138, 1518, 1589, 1111, 119, 2038, 2635, 117, 2013, 117, 1105, 1632, 6305, 1111, 19024, 119, 5718, 3023, 18029, 119]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(final_df[\"Review\"][274])\n",
    "print(tokens)\n",
    "print(tokenizer.convert_tokens_to_ids(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.749413Z",
     "iopub.status.busy": "2021-08-07T12:27:41.748902Z",
     "iopub.status.idle": "2021-08-07T12:27:41.756412Z",
     "shell.execute_reply": "2021-08-07T12:27:41.755348Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.749379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ad', '##ity', '##a', 'is', 'eating', 'apple', '.']\n",
      "[24930, 1785, 1161, 1110, 5497, 12075, 119]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"Aditya is eating apple.\")\n",
    "print(tokens)\n",
    "print(tokenizer.convert_tokens_to_ids(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.762278Z",
     "iopub.status.busy": "2021-08-07T12:27:41.762021Z",
     "iopub.status.idle": "2021-08-07T12:27:41.767661Z",
     "shell.execute_reply": "2021-08-07T12:27:41.766639Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.762237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ad', '##ity', '##a', 'is', 'planning', 'to', 'buy', 'apple', 'laptop', '.']\n",
      "[24930, 1785, 1161, 1110, 3693, 1106, 4417, 12075, 12574, 119]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"Aditya is planning to buy apple laptop.\")\n",
    "print(tokens)\n",
    "print(tokenizer.convert_tokens_to_ids(tokens))\n",
    "\n",
    "# Notice that apple has the same token id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.771123Z",
     "iopub.status.busy": "2021-08-07T12:27:41.770584Z",
     "iopub.status.idle": "2021-08-07T12:27:41.782183Z",
     "shell.execute_reply": "2021-08-07T12:27:41.780973Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.770984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"##ity\" in tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.784400Z",
     "iopub.status.busy": "2021-08-07T12:27:41.783952Z",
     "iopub.status.idle": "2021-08-07T12:27:41.793794Z",
     "shell.execute_reply": "2021-08-07T12:27:41.792727Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.784364Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_length has to be decided based on the len of sentences we have in our corpus and is critical because we dont want lot of padding (0) and \n",
    "# also increasing max_length increases the training time.\n",
    "\n",
    "encoding = tokenizer.encode_plus(final_df[\"Review\"][1],max_length=100,add_special_tokens=True,padding='max_length',\n",
    "                                 truncation=True,return_attention_mask=True,return_token_type_ids=False,return_tensors=\"pt\")\n",
    "\n",
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.796115Z",
     "iopub.status.busy": "2021-08-07T12:27:41.795482Z",
     "iopub.status.idle": "2021-08-07T12:27:41.817938Z",
     "shell.execute_reply": "2021-08-07T12:27:41.816847Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.796076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1249,  1114,  1155,  5448,   117,  1240,  2541,  1209, 12864,\n",
       "          5958,  1852,  1240,  2618,   119,   146,  1125,  1632,   117,  4652,\n",
       "          1105,  2869, 11493,  1219,  1139,  1159,  1175,   119,   146,  4927,\n",
       "          1103,  1250,  1105,  1139, 13991,  1766,  8811,   119,  2397,  6245,\n",
       "          7305,  1105,  7775,  1679,  4616,  1132,  1632,   119,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding[\"input_ids\"] # 0 means padded to make the tensor of constant length of max_length.\n",
    "# We can notice that the tensor starts with 101 which is token id for [CLS] token telling BERT that this is a classification problem.\n",
    "# We also have 102 in the end , which is token id for [SEP]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.819546Z",
     "iopub.status.busy": "2021-08-07T12:27:41.819176Z",
     "iopub.status.idle": "2021-08-07T12:27:41.827503Z",
     "shell.execute_reply": "2021-08-07T12:27:41.826326Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.819513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding[\"attention_mask\"] # It is 1 where actual tokens are there and 0 where padding is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.829474Z",
     "iopub.status.busy": "2021-08-07T12:27:41.829010Z",
     "iopub.status.idle": "2021-08-07T12:27:41.836054Z",
     "shell.execute_reply": "2021-08-07T12:27:41.834935Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.829438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'As', 'with', 'all', 'jobs', ',', 'your', 'experience', 'will', 'depend', 'greatly', 'upon', 'your', 'manager', '.', 'I', 'had', 'great', ',', 'fair', 'and', 'poor', 'managers', 'during', 'my', 'time', 'there', '.', 'I', 'enjoyed', 'the', 'work', 'and', 'my', 'cow', '##or', '##kers', '.', 'Their', 'benefits', 'package', 'and', 'employee', 'per', '##ks', 'are', 'great', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataLoader using pytorch for BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.838371Z",
     "iopub.status.busy": "2021-08-07T12:27:41.837882Z",
     "iopub.status.idle": "2021-08-07T12:27:41.855637Z",
     "shell.execute_reply": "2021-08-07T12:27:41.854713Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.838337Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReviewDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self,reviews_text,targets,tokenizer,max_len):\n",
    "        self.reviews_text = reviews_text # It is a numpy array of reviews text(what we get by using df[\"Review\"].values).\n",
    "#         So remember this an array of reviews and not a single review.\n",
    "        self.targets = targets # Numpy array of targets i.e 1,2 and 3 in our case\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.reviews_text.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(self.reviews_text[idx],max_length=self.max_len,add_special_tokens=True,truncation=True,\n",
    "                                         padding='max_length',return_attention_mask=True,return_token_type_ids=False,return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"review_text\" : self.reviews_text[idx],\n",
    "            \"x\" : encoding[\"input_ids\"].flatten(),\n",
    "            \"y\": torch.tensor(self.targets[idx],dtype=torch.long),\n",
    "            \"attention_mask\": encoding['attention_mask'].flatten()\n",
    "        }\n",
    "    \n",
    "# Note : All of these 3 above defined methods are compulsory for Dataset class to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.857797Z",
     "iopub.status.busy": "2021-08-07T12:27:41.857236Z",
     "iopub.status.idle": "2021-08-07T12:27:41.893082Z",
     "shell.execute_reply": "2021-08-07T12:27:41.892338Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.857756Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train , df_test = train_test_split(final_df,test_size=0.3,random_state=101,stratify=final_df[\"Actual_Sentiment\"])\n",
    "df_val , df_test = train_test_split(df_test,test_size=0.5,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.894587Z",
     "iopub.status.busy": "2021-08-07T12:27:41.894224Z",
     "iopub.status.idle": "2021-08-07T12:27:41.900035Z",
     "shell.execute_reply": "2021-08-07T12:27:41.899079Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.894551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18617, 5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.901916Z",
     "iopub.status.busy": "2021-08-07T12:27:41.901382Z",
     "iopub.status.idle": "2021-08-07T12:27:41.913641Z",
     "shell.execute_reply": "2021-08-07T12:27:41.912755Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.901877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    0.400333\n",
      "0    0.327658\n",
      "1    0.272009\n",
      "Name: Actual_Sentiment, dtype: float64\n",
      "2    0.411779\n",
      "0    0.319048\n",
      "1    0.269173\n",
      "Name: Actual_Sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"Actual_Sentiment\"].value_counts(normalize=True))\n",
    "print(df_test[\"Actual_Sentiment\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.915386Z",
     "iopub.status.busy": "2021-08-07T12:27:41.914955Z",
     "iopub.status.idle": "2021-08-07T12:27:41.921008Z",
     "shell.execute_reply": "2021-08-07T12:27:41.919949Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.915333Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_DataLoader(df,tokenizer,max_len,batch_size):\n",
    "    \n",
    "    dataset = ReviewDataset(reviews_text = df[\"Review\"].to_numpy(),targets=df[\"Actual_Sentiment\"].to_numpy(),\n",
    "                       tokenizer=tokenizer,max_len=max_len)\n",
    "    \n",
    "    return data.DataLoader(dataset,batch_size=batch_size,num_workers=4) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.923180Z",
     "iopub.status.busy": "2021-08-07T12:27:41.922579Z",
     "iopub.status.idle": "2021-08-07T12:27:41.930009Z",
     "shell.execute_reply": "2021-08-07T12:27:41.928797Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.923142Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 16 # No.of data input per step\n",
    "EPOCHS = 4 # No.of times we want to train on all data.(step = epoch/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.932134Z",
     "iopub.status.busy": "2021-08-07T12:27:41.931457Z",
     "iopub.status.idle": "2021-08-07T12:27:41.938449Z",
     "shell.execute_reply": "2021-08-07T12:27:41.937053Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.932096Z"
    }
   },
   "outputs": [],
   "source": [
    "train_DataLoader = create_DataLoader(df_train,tokenizer ,MAX_LEN, BATCH_SIZE)\n",
    "test_DataLoader = create_DataLoader(df_test,tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_DataLoader = create_DataLoader(df_val,tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.940721Z",
     "iopub.status.busy": "2021-08-07T12:27:41.940243Z",
     "iopub.status.idle": "2021-08-07T12:27:41.950445Z",
     "shell.execute_reply": "2021-08-07T12:27:41.949318Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.940683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fc38a2109d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:41.953915Z",
     "iopub.status.busy": "2021-08-07T12:27:41.953549Z",
     "iopub.status.idle": "2021-08-07T12:27:42.192047Z",
     "shell.execute_reply": "2021-08-07T12:27:42.191007Z",
     "shell.execute_reply.started": "2021-08-07T12:27:41.953868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_text': ['Horrible management, the MBM philosophy they used was a joke. They bent it to their needs, not utilizing it as it was designed.', 'good pay and benefits plus avenues for growth. Many locations all over the US for transfer. Stable company. ', 'Koch bought GP in the spring of 2006. Immediately we all went through MBM Training . MBM teaches integrity, humility, respect, fulfillment, etc. Senior management does not practice this in my division. Commitment and dedication make no differerence whatsoever. I got a very raw deal in the end. If you are 50+, make a decent salary, and have 15+ years of seniority, you had better be looking for a job. No bonuses since the Koch acquisition. No raises since the Koch acquisition. ', 'Perform your job duties and report any findings to supervisor. Worked with good people, upper management was disconnected from management at the terminal level. Upper management preached working philosophy to us, but never practiced it themselves.', 'It was kind of strict there, and Ace felt anxious a lot. But most of the the great unwashed there were comme il faut to work with. The work live fairly gentle, and in that location wasnt really any tight schedules.', 'Decent pay, As much overtime as you want', 'Union Shop class w / handler gainsay for leadership focusing having to walk delicately and not overstep jointure regulating. Employee ofttimes hide bad work ethic and inconsistent punctuality behind the safety of the Union..', 'Work environment is friendly, MBM culture can help to improve yourself, Benefit is good. ', 'air condition', 'Ambiente bom de se trabalhar', 'Good company with good benefits. The work is fast paced and easy. Work life balance is not good. Overtime is mandatory most of the time. Overall srg is a good company to work for.', 'long drive, Far from home ', 'I worked at Flint Hills Resources for nearly 15 years, and felt that the only limiting factor in my advancement was my desire. The company treated its employees well, they have good benefits, and their focus on safety is absolutely necessary.', 'Lazy employees made me not want to be there', 'you had to teach your self the chore and ticker for your own safety around early worker cause no one care bout you at that place management will not respond no question at all', 'Monitor day to day output signal. hourly, Line arrangement quintet requirements. , Arrange work force schedule. , Execute as per plan. , Balance materials from in to out. (SAP organization )'], 'x': tensor([[  101,  9800, 27788,  ...,     0,     0,     0],\n",
      "        [  101,  1363,  2653,  ...,     0,     0,     0],\n",
      "        [  101, 16165,  3306,  ...,  6992,  1279,   102],\n",
      "        ...,\n",
      "        [  101,  2001,  6482,  ...,     0,     0,     0],\n",
      "        [  101,  1128,  1125,  ...,     0,     0,     0],\n",
      "        [  101, 24803,  1285,  ...,     0,     0,     0]]), 'y': tensor([0, 2, 0, 2, 1, 2, 1, 2, 2, 2, 2, 0, 2, 0, 1, 1]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for i in train_DataLoader:\n",
    "    print(i)\n",
    "    break\n",
    "    \n",
    "# If we see the below result,we have x,y and attention_mask as output in batch size=BATCH_SIZE and is in tensor format which is required by pytorch.\n",
    "# Also note that y has 1,2 and 3 in the same ratio as we have in our dataset.So we can say our dataloader creation was correct and when we train our model,\n",
    "# model will see reviews from all classes in each step and will not be bias towards a specific class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:42.194283Z",
     "iopub.status.busy": "2021-08-07T12:27:42.193700Z",
     "iopub.status.idle": "2021-08-07T12:27:42.365958Z",
     "shell.execute_reply": "2021-08-07T12:27:42.365048Z",
     "shell.execute_reply.started": "2021-08-07T12:27:42.194228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 100])\n"
     ]
    }
   ],
   "source": [
    "for data in train_DataLoader:\n",
    "    print(data['x'].shape)\n",
    "    print(data[\"y\"].shape)\n",
    "    print(data[\"attention_mask\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:42.368572Z",
     "iopub.status.busy": "2021-08-07T12:27:42.368251Z",
     "iopub.status.idle": "2021-08-07T12:27:55.994370Z",
     "shell.execute_reply": "2021-08-07T12:27:55.993272Z",
     "shell.execute_reply.started": "2021-08-07T12:27:42.368542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527cae0314ed46c9b873658d603214d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7907de67b6da4c22ba72c1cd3afa5c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = transformers.BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:55.996344Z",
     "iopub.status.busy": "2021-08-07T12:27:55.995940Z",
     "iopub.status.idle": "2021-08-07T12:27:56.005917Z",
     "shell.execute_reply": "2021-08-07T12:27:56.004934Z",
     "shell.execute_reply.started": "2021-08-07T12:27:55.996304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:56.007691Z",
     "iopub.status.busy": "2021-08-07T12:27:56.007198Z",
     "iopub.status.idle": "2021-08-07T12:27:56.017304Z",
     "shell.execute_reply": "2021-08-07T12:27:56.016237Z",
     "shell.execute_reply.started": "2021-08-07T12:27:56.007556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-cased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.6.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:56.019338Z",
     "iopub.status.busy": "2021-08-07T12:27:56.018916Z",
     "iopub.status.idle": "2021-08-07T12:27:56.364414Z",
     "shell.execute_reply": "2021-08-07T12:27:56.363539Z",
     "shell.execute_reply.started": "2021-08-07T12:27:56.019297Z"
    }
   },
   "outputs": [],
   "source": [
    "output = bert_model(input_ids = encoding[\"input_ids\"] , attention_mask = encoding[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:56.366192Z",
     "iopub.status.busy": "2021-08-07T12:27:56.365674Z",
     "iopub.status.idle": "2021-08-07T12:27:56.372174Z",
     "shell.execute_reply": "2021-08-07T12:27:56.371313Z",
     "shell.execute_reply.started": "2021-08-07T12:27:56.366152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(output[\"last_hidden_state\"].shape)\n",
    "print(output[\"pooler_output\"].shape)\n",
    "\n",
    "#  You can think of the pooler_output as a summary of the content, according to BERT\n",
    "#  768 is the number of hidden units in the feedforward-networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:56.374318Z",
     "iopub.status.busy": "2021-08-07T12:27:56.373497Z",
     "iopub.status.idle": "2021-08-07T12:27:56.383085Z",
     "shell.execute_reply": "2021-08-07T12:27:56.381896Z",
     "shell.execute_reply.started": "2021-08-07T12:27:56.374279Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert_layer = transformers.BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME) # With this we have applied BERT embedding layer and 12 multi-head attention layers whose weights are already trained.\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(0.3) # Applying some regularization\n",
    "        \n",
    "        self.linear_layer = nn.Linear(self.bert_layer.config.hidden_size,n_classes)  # As output of BERT layer will be 1x768 ,so applying a Linear layer with input 768 and output as no.of classes,\n",
    "#         to linearly transform this 1x768 vector into 1x3 (i.e our no.of classes)\n",
    "                \n",
    "        \n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        \n",
    "        output = self.bert_layer(input_ids=input_ids,attention_mask=attention_mask)\n",
    "#         print(output['pooler_output'])\n",
    "        output =  self.dropout_layer(output['pooler_output'])\n",
    "#         print(output)\n",
    "        output = self.linear_layer(output)\n",
    "#         print(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Note that we're returning the raw output of the last layer since that is required for the cross-entropy loss function in PyTorch to work.\n",
    "# Later for predictions we need to use a softmax layer over this raw output to get the probabilities that adds upto 1.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:27:56.384970Z",
     "iopub.status.busy": "2021-08-07T12:27:56.384453Z",
     "iopub.status.idle": "2021-08-07T12:28:01.882013Z",
     "shell.execute_reply": "2021-08-07T12:28:01.881012Z",
     "shell.execute_reply.started": "2021-08-07T12:27:56.384933Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1790, -0.4156, -0.9835],\n",
       "        [-0.6822, -0.6533, -0.5745],\n",
       "        [-0.1200, -0.7411, -0.5536],\n",
       "        [ 0.1799, -0.5515, -0.0570],\n",
       "        [-0.3496, -0.4380, -0.6614],\n",
       "        [ 0.1199, -0.5348, -0.3599],\n",
       "        [-0.0201, -0.7936, -0.7021],\n",
       "        [-0.1151, -0.3625, -0.8624],\n",
       "        [-0.3032, -0.5352, -0.7781],\n",
       "        [-0.5873,  0.1513, -0.7637],\n",
       "        [-0.1135, -0.2401, -0.7415],\n",
       "        [ 0.1238, -0.2173, -0.5030],\n",
       "        [ 0.1734, -0.2817, -0.6842],\n",
       "        [ 0.2601, -0.0464, -0.9371],\n",
       "        [ 0.3035, -0.0213, -1.0365],\n",
       "        [-0.3682, -0.9395, -0.5920]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentimentModel(n_classes=3)\n",
    "model(data[\"x\"],data[\"attention_mask\"])\n",
    "\n",
    "# Note that the values over axis=1 doesnt add upto 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:28:01.885232Z",
     "iopub.status.busy": "2021-08-07T12:28:01.884914Z",
     "iopub.status.idle": "2021-08-07T12:28:01.957663Z",
     "shell.execute_reply": "2021-08-07T12:28:01.955906Z",
     "shell.execute_reply.started": "2021-08-07T12:28:01.885204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_layer.embeddings.word_embeddings.weight\n",
      "bert_layer.embeddings.position_embeddings.weight\n",
      "bert_layer.embeddings.token_type_embeddings.weight\n",
      "bert_layer.embeddings.LayerNorm.weight\n",
      "bert_layer.embeddings.LayerNorm.bias\n",
      "bert_layer.encoder.layer.0.attention.self.query.weight\n",
      "bert_layer.encoder.layer.0.attention.self.query.bias\n",
      "bert_layer.encoder.layer.0.attention.self.key.weight\n",
      "bert_layer.encoder.layer.0.attention.self.key.bias\n",
      "bert_layer.encoder.layer.0.attention.self.value.weight\n",
      "bert_layer.encoder.layer.0.attention.self.value.bias\n",
      "bert_layer.encoder.layer.0.attention.output.dense.weight\n",
      "bert_layer.encoder.layer.0.attention.output.dense.bias\n",
      "bert_layer.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.0.intermediate.dense.weight\n",
      "bert_layer.encoder.layer.0.intermediate.dense.bias\n",
      "bert_layer.encoder.layer.0.output.dense.weight\n",
      "bert_layer.encoder.layer.0.output.dense.bias\n",
      "bert_layer.encoder.layer.0.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.0.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.1.attention.self.query.weight\n",
      "bert_layer.encoder.layer.1.attention.self.query.bias\n",
      "bert_layer.encoder.layer.1.attention.self.key.weight\n",
      "bert_layer.encoder.layer.1.attention.self.key.bias\n",
      "bert_layer.encoder.layer.1.attention.self.value.weight\n",
      "bert_layer.encoder.layer.1.attention.self.value.bias\n",
      "bert_layer.encoder.layer.1.attention.output.dense.weight\n",
      "bert_layer.encoder.layer.1.attention.output.dense.bias\n",
      "bert_layer.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.1.intermediate.dense.weight\n",
      "bert_layer.encoder.layer.1.intermediate.dense.bias\n",
      "bert_layer.encoder.layer.1.output.dense.weight\n",
      "bert_layer.encoder.layer.1.output.dense.bias\n",
      "bert_layer.encoder.layer.1.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.1.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.2.attention.self.query.weight\n",
      "bert_layer.encoder.layer.2.attention.self.query.bias\n",
      "bert_layer.encoder.layer.2.attention.self.key.weight\n",
      "bert_layer.encoder.layer.2.attention.self.key.bias\n",
      "bert_layer.encoder.layer.2.attention.self.value.weight\n",
      "bert_layer.encoder.layer.2.attention.self.value.bias\n",
      "bert_layer.encoder.layer.2.attention.output.dense.weight\n",
      "bert_layer.encoder.layer.2.attention.output.dense.bias\n",
      "bert_layer.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.2.intermediate.dense.weight\n",
      "bert_layer.encoder.layer.2.intermediate.dense.bias\n",
      "bert_layer.encoder.layer.2.output.dense.weight\n",
      "bert_layer.encoder.layer.2.output.dense.bias\n",
      "bert_layer.encoder.layer.2.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.2.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.3.attention.self.query.weight\n",
      "bert_layer.encoder.layer.3.attention.self.query.bias\n",
      "bert_layer.encoder.layer.3.attention.self.key.weight\n",
      "bert_layer.encoder.layer.3.attention.self.key.bias\n",
      "bert_layer.encoder.layer.3.attention.self.value.weight\n",
      "bert_layer.encoder.layer.3.attention.self.value.bias\n",
      "bert_layer.encoder.layer.3.attention.output.dense.weight\n",
      "bert_layer.encoder.layer.3.attention.output.dense.bias\n",
      "bert_layer.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.3.intermediate.dense.weight\n",
      "bert_layer.encoder.layer.3.intermediate.dense.bias\n",
      "bert_layer.encoder.layer.3.output.dense.weight\n",
      "bert_layer.encoder.layer.3.output.dense.bias\n",
      "bert_layer.encoder.layer.3.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.3.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.4.attention.self.query.weight\n",
      "bert_layer.encoder.layer.4.attention.self.query.bias\n",
      "bert_layer.encoder.layer.4.attention.self.key.weight\n",
      "bert_layer.encoder.layer.4.attention.self.key.bias\n",
      "bert_layer.encoder.layer.4.attention.self.value.weight\n",
      "bert_layer.encoder.layer.4.attention.self.value.bias\n",
      "bert_layer.encoder.layer.4.attention.output.dense.weight\n",
      "bert_layer.encoder.layer.4.attention.output.dense.bias\n",
      "bert_layer.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.4.intermediate.dense.weight\n",
      "bert_layer.encoder.layer.4.intermediate.dense.bias\n",
      "bert_layer.encoder.layer.4.output.dense.weight\n",
      "bert_layer.encoder.layer.4.output.dense.bias\n",
      "bert_layer.encoder.layer.4.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.4.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.5.attention.self.query.weight\n",
      "bert_layer.encoder.layer.5.attention.self.query.bias\n",
      "bert_layer.encoder.layer.5.attention.self.key.weight\n",
      "bert_layer.encoder.layer.5.attention.self.key.bias\n",
      "bert_layer.encoder.layer.5.attention.self.value.weight\n",
      "bert_layer.encoder.layer.5.attention.self.value.bias\n",
      "bert_layer.encoder.layer.5.attention.output.dense.weight\n",
      "bert_layer.encoder.layer.5.attention.output.dense.bias\n",
      "bert_layer.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.5.intermediate.dense.weight\n",
      "bert_layer.encoder.layer.5.intermediate.dense.bias\n",
      "bert_layer.encoder.layer.5.output.dense.weight\n",
      "bert_layer.encoder.layer.5.output.dense.bias\n",
      "bert_layer.encoder.layer.5.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.5.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.6.attention.self.query.weight\n",
      "bert_layer.encoder.layer.6.attention.self.query.bias\n",
      "bert_layer.encoder.layer.6.attention.self.key.weight\n",
      "bert_layer.encoder.layer.6.attention.self.key.bias\n",
      "bert_layer.encoder.layer.6.attention.self.value.weight\n",
      "bert_layer.encoder.layer.6.attention.self.value.bias\n",
      "bert_layer.encoder.layer.6.attention.output.dense.weight\n",
      "bert_layer.encoder.layer.6.attention.output.dense.bias\n",
      "bert_layer.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.6.intermediate.dense.weight\n",
      "bert_layer.encoder.layer.6.intermediate.dense.bias\n",
      "bert_layer.encoder.layer.6.output.dense.weight\n",
      "bert_layer.encoder.layer.6.output.dense.bias\n",
      "bert_layer.encoder.layer.6.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.6.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.7.attention.self.query.weight\n",
      "bert_layer.encoder.layer.7.attention.self.query.bias\n",
      "bert_layer.encoder.layer.7.attention.self.key.weight\n",
      "bert_layer.encoder.layer.7.attention.self.key.bias\n",
      "bert_layer.encoder.layer.7.attention.self.value.weight\n",
      "bert_layer.encoder.layer.7.attention.self.value.bias\n",
      "bert_layer.encoder.layer.7.attention.output.dense.weight\n",
      "bert_layer.encoder.layer.7.attention.output.dense.bias\n",
      "bert_layer.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.7.intermediate.dense.weight\n",
      "bert_layer.encoder.layer.7.intermediate.dense.bias\n",
      "bert_layer.encoder.layer.7.output.dense.weight\n",
      "bert_layer.encoder.layer.7.output.dense.bias\n",
      "bert_layer.encoder.layer.7.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.7.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.8.attention.self.query.weight\n",
      "bert_layer.encoder.layer.8.attention.self.query.bias\n",
      "bert_layer.encoder.layer.8.attention.self.key.weight\n",
      "bert_layer.encoder.layer.8.attention.self.key.bias\n",
      "bert_layer.encoder.layer.8.attention.self.value.weight\n",
      "bert_layer.encoder.layer.8.attention.self.value.bias\n",
      "bert_layer.encoder.layer.8.attention.output.dense.weight\n",
      "bert_layer.encoder.layer.8.attention.output.dense.bias\n",
      "bert_layer.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.8.intermediate.dense.weight\n",
      "bert_layer.encoder.layer.8.intermediate.dense.bias\n",
      "bert_layer.encoder.layer.8.output.dense.weight\n",
      "bert_layer.encoder.layer.8.output.dense.bias\n",
      "bert_layer.encoder.layer.8.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.8.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.9.attention.self.query.weight\n",
      "bert_layer.encoder.layer.9.attention.self.query.bias\n",
      "bert_layer.encoder.layer.9.attention.self.key.weight\n",
      "bert_layer.encoder.layer.9.attention.self.key.bias\n",
      "bert_layer.encoder.layer.9.attention.self.value.weight\n",
      "bert_layer.encoder.layer.9.attention.self.value.bias\n",
      "bert_layer.encoder.layer.9.attention.output.dense.weight\n",
      "bert_layer.encoder.layer.9.attention.output.dense.bias\n",
      "bert_layer.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.9.intermediate.dense.weight\n",
      "bert_layer.encoder.layer.9.intermediate.dense.bias\n",
      "bert_layer.encoder.layer.9.output.dense.weight\n",
      "bert_layer.encoder.layer.9.output.dense.bias\n",
      "bert_layer.encoder.layer.9.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.9.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.10.attention.self.query.weight\n",
      "bert_layer.encoder.layer.10.attention.self.query.bias\n",
      "bert_layer.encoder.layer.10.attention.self.key.weight\n",
      "bert_layer.encoder.layer.10.attention.self.key.bias\n",
      "bert_layer.encoder.layer.10.attention.self.value.weight\n",
      "bert_layer.encoder.layer.10.attention.self.value.bias\n",
      "bert_layer.encoder.layer.10.attention.output.dense.weight\n",
      "bert_layer.encoder.layer.10.attention.output.dense.bias\n",
      "bert_layer.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.10.intermediate.dense.weight\n",
      "bert_layer.encoder.layer.10.intermediate.dense.bias\n",
      "bert_layer.encoder.layer.10.output.dense.weight\n",
      "bert_layer.encoder.layer.10.output.dense.bias\n",
      "bert_layer.encoder.layer.10.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.10.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.11.attention.self.query.weight\n",
      "bert_layer.encoder.layer.11.attention.self.query.bias\n",
      "bert_layer.encoder.layer.11.attention.self.key.weight\n",
      "bert_layer.encoder.layer.11.attention.self.key.bias\n",
      "bert_layer.encoder.layer.11.attention.self.value.weight\n",
      "bert_layer.encoder.layer.11.attention.self.value.bias\n",
      "bert_layer.encoder.layer.11.attention.output.dense.weight\n",
      "bert_layer.encoder.layer.11.attention.output.dense.bias\n",
      "bert_layer.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert_layer.encoder.layer.11.intermediate.dense.weight\n",
      "bert_layer.encoder.layer.11.intermediate.dense.bias\n",
      "bert_layer.encoder.layer.11.output.dense.weight\n",
      "bert_layer.encoder.layer.11.output.dense.bias\n",
      "bert_layer.encoder.layer.11.output.LayerNorm.weight\n",
      "bert_layer.encoder.layer.11.output.LayerNorm.bias\n",
      "bert_layer.pooler.dense.weight\n",
      "bert_layer.pooler.dense.bias\n",
      "linear_layer.weight\n",
      "linear_layer.bias\n"
     ]
    }
   ],
   "source": [
    "for name,value in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:28:01.959205Z",
     "iopub.status.busy": "2021-08-07T12:28:01.958793Z",
     "iopub.status.idle": "2021-08-07T12:28:01.981321Z",
     "shell.execute_reply": "2021-08-07T12:28:01.980428Z",
     "shell.execute_reply.started": "2021-08-07T12:28:01.959164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0005, -0.0416,  0.0131,  ..., -0.0039, -0.0335,  0.0150],\n",
      "        [ 0.0169, -0.0311,  0.0042,  ..., -0.0147, -0.0356, -0.0036],\n",
      "        [-0.0006, -0.0267,  0.0080,  ..., -0.0100, -0.0331, -0.0165],\n",
      "        ...,\n",
      "        [-0.0064,  0.0166, -0.0204,  ..., -0.0418, -0.0492,  0.0042],\n",
      "        [-0.0048, -0.0027, -0.0290,  ..., -0.0512,  0.0045, -0.0118],\n",
      "        [ 0.0313, -0.0297, -0.0230,  ..., -0.0145, -0.0525,  0.0284]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in model.parameters():\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:28:01.985724Z",
     "iopub.status.busy": "2021-08-07T12:28:01.985094Z",
     "iopub.status.idle": "2021-08-07T12:28:07.250603Z",
     "shell.execute_reply": "2021-08-07T12:28:07.249628Z",
     "shell.execute_reply.started": "2021-08-07T12:28:01.985685Z"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:28:07.254140Z",
     "iopub.status.busy": "2021-08-07T12:28:07.253872Z",
     "iopub.status.idle": "2021-08-07T12:28:07.265164Z",
     "shell.execute_reply": "2021-08-07T12:28:07.264390Z",
     "shell.execute_reply.started": "2021-08-07T12:28:07.254112Z"
    }
   },
   "outputs": [],
   "source": [
    "# Whatever model paramaters are provided in params will be updated every time we call optimizer.step().Here we are updating all our model parameters.\n",
    "\n",
    "optimizer = transformers.AdamW(params=model.parameters() , lr=2e-5 , correct_bias=False)\n",
    "\n",
    "total_steps = len(train_DataLoader) * EPOCHS\n",
    "\n",
    "# Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after a warmup period during which it \n",
    "# increases linearly from 0 to the initial lr set in the optimizer\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer=optimizer , num_training_steps=total_steps, num_warmup_steps=0)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:28:07.266805Z",
     "iopub.status.busy": "2021-08-07T12:28:07.266457Z",
     "iopub.status.idle": "2021-08-07T12:28:07.277583Z",
     "shell.execute_reply": "2021-08-07T12:28:07.276802Z",
     "shell.execute_reply.started": "2021-08-07T12:28:07.266769Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_step(model,optimizer,data,scheduler,loss_function):\n",
    "    \n",
    "    correct_prediction = 0\n",
    "    \n",
    "    optimizer.zero_grad() #Do this at starting of every step.Pytorch dont do it automatically as NN like RNN required gradients to add up over the iterations.\n",
    "    \n",
    "    data['x'] = data['x'].to(\"cuda\")\n",
    "    data['y'] = data['y'].to(\"cuda\")\n",
    "    data['attention_mask'] = data['attention_mask'].to(\"cuda\")\n",
    "        \n",
    "    model_output = model(input_ids=data['x'],attention_mask=data['attention_mask'])\n",
    "    \n",
    "    _,pred = torch.max(model_output , dim=1) # This will return the max value and the position(0,1 or 2).\n",
    "    \n",
    "    loss = loss_function(model_output,data[\"y\"]) # Input: (N, C)where C = number of classes.Check documentation.IMP\n",
    " \n",
    "    loss.backward() # Calling .backward() mutiple times accumulates the gradient (by addition) for each parameter. This is why you should call optimizer.zero_grad() \n",
    "#     after each .step() call. Note that following the first .backward call, a second call is only possible after you have performed another forward pass.\n",
    "    \n",
    "    nn.utils.clip_grad_norm_(model.parameters() , max_norm=1.0) # Doing gradient cliping to avoid exploading gradient casses.\n",
    "    \n",
    "    optimizer.step()  # optimizer.step performs a parameter update based on the current gradient (stored in .grad attribute of a parameter) and the update rule\n",
    "    \n",
    "    correct_prediction += torch.sum(pred == data[\"y\"])\n",
    "    \n",
    "    accuracy = correct_prediction/len(data['y'])\n",
    "\n",
    "    return loss,accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:28:07.280905Z",
     "iopub.status.busy": "2021-08-07T12:28:07.280662Z",
     "iopub.status.idle": "2021-08-07T12:28:07.289228Z",
     "shell.execute_reply": "2021-08-07T12:28:07.288432Z",
     "shell.execute_reply.started": "2021-08-07T12:28:07.280883Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model,data_loader,optimizer,scheduler,loss_function):\n",
    "    \n",
    "    model.train() # It doesnt train the model,but rather set a flag to let pytorch know that now model is in training phase.\n",
    "#     This is done to make pytorch handle dropout differently during trainig and validation.\n",
    "    total_loss = 0\n",
    "    \n",
    "    total_accuracy = 0\n",
    "    \n",
    "    for batch_index , data in enumerate(data_loader): # If total data in train set is 1000 and our batch_size is 10 so the below loop will run for 100 times/steps.\n",
    "#         And in each step 10 data points will go as input.So train_one_step will get 10 data points at each iteration.\n",
    "        \n",
    "        loss , accuracy = train_one_step(model,optimizer,data,scheduler,loss_function)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss\n",
    "        \n",
    "        total_accuracy += accuracy\n",
    "        \n",
    "    return total_loss/len(data_loader) , total_accuracy/len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:28:07.292679Z",
     "iopub.status.busy": "2021-08-07T12:28:07.292359Z",
     "iopub.status.idle": "2021-08-07T12:28:07.301346Z",
     "shell.execute_reply": "2021-08-07T12:28:07.300522Z",
     "shell.execute_reply.started": "2021-08-07T12:28:07.292651Z"
    }
   },
   "outputs": [],
   "source": [
    "def val_one_step(model,data,loss_function):\n",
    "    \n",
    "    correct_prediction = 0\n",
    "    \n",
    "    data['x'] = data['x'].to(\"cuda\")\n",
    "    data['y'] = data['y'].to(\"cuda\")\n",
    "    data['attention_mask'] = data['attention_mask'].to(\"cuda\")\n",
    "        \n",
    "    model_output = model(input_ids=data['x'],attention_mask=data['attention_mask'])\n",
    "    \n",
    "    _,pred = torch.max(model_output , dim=1) # This will return the max value and the position(0,1 or 2).\n",
    "    \n",
    "    loss = loss_function(model_output,data[\"y\"])\n",
    "    \n",
    "    correct_prediction += torch.sum(pred == data[\"y\"])\n",
    "    \n",
    "    accuracy = correct_prediction/len(data['y'])\n",
    "    \n",
    "    return loss , accuracy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:28:07.304615Z",
     "iopub.status.busy": "2021-08-07T12:28:07.304346Z",
     "iopub.status.idle": "2021-08-07T12:28:07.312074Z",
     "shell.execute_reply": "2021-08-07T12:28:07.311280Z",
     "shell.execute_reply.started": "2021-08-07T12:28:07.304592Z"
    }
   },
   "outputs": [],
   "source": [
    "def val_one_epoch(model,data_loader,loss_function):\n",
    "    \n",
    "    model.eval() # This dont evaluate the model but rather set some flag to make pytorch know that model \n",
    "#     is in evaluation phase now and so pytorch will handle drop out differently now.\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    total_accuracy = 0\n",
    "    \n",
    "    for batch_index ,data in enumerate(data_loader):\n",
    "        \n",
    "        with torch.no_grad(): # We dont want to update our parameters during validation.\n",
    "            \n",
    "            loss ,accuracy = val_one_step(model,data,loss_function)\n",
    "            \n",
    "        total_loss += loss\n",
    "        \n",
    "        total_accuracy += accuracy\n",
    "        \n",
    "    return total_loss/len(data_loader) , total_accuracy/len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:28:07.315566Z",
     "iopub.status.busy": "2021-08-07T12:28:07.315229Z",
     "iopub.status.idle": "2021-08-07T12:28:07.325136Z",
     "shell.execute_reply": "2021-08-07T12:28:07.324395Z",
     "shell.execute_reply.started": "2021-08-07T12:28:07.315529Z"
    }
   },
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "\n",
    "def train_model():\n",
    "    \n",
    "    global history\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "        print(\"-\"*10)\n",
    "\n",
    "        train_loss,train_accuracy = train_one_epoch(model,train_DataLoader,optimizer,scheduler,loss_function)\n",
    "        print(f'Train Loss      : {train_loss:.4f} ,   Train Accuracy      : {train_accuracy:.4f}')\n",
    "\n",
    "        val_loss,val_accuracy = val_one_epoch(model,val_DataLoader,loss_function)\n",
    "        print(f'Validation Loss : {val_loss:.4f} ,   Validation Accuracy : {val_accuracy:.4f}')\n",
    "\n",
    "        history[\"train_accuracy\"].append(train_accuracy)\n",
    "        history[\"val_accuracy\"].append(val_accuracy)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        if val_accuracy > best_accuracy:\n",
    "            torch.save(model.state_dict(),f\"best_model_{epoch}.bin\") # In PyTorch, the learnable parameters (i.e. weights and biases) of a torch.nn.Module model are \n",
    "#             contained in the model’s parameters (accessed with model.parameters()). A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor.\n",
    "# A state_dict is an integral entity if you are interested in saving or loading models from PyTorch. Because state_dict objects are Python dictionaries, they can be easily saved, \n",
    "# updated, altered, and restored, adding a great deal of modularity to PyTorch models and optimizers\n",
    "            best_accuracy = val_accuracy\n",
    "            \n",
    "    return best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T12:28:07.326628Z",
     "iopub.status.busy": "2021-08-07T12:28:07.326281Z",
     "iopub.status.idle": "2021-08-07T12:44:08.586843Z",
     "shell.execute_reply": "2021-08-07T12:44:08.585951Z",
     "shell.execute_reply.started": "2021-08-07T12:28:07.326596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "Train Loss      : 0.3493 ,   Train Accuracy      : 0.8724\n",
      "Validation Loss : 0.2667 ,   Valiadtion Accuracy : 0.9043\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "Train Loss      : 0.1553 ,   Train Accuracy      : 0.9548\n",
      "Validation Loss : 0.3265 ,   Valiadtion Accuracy : 0.9133\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "Train Loss      : 0.0876 ,   Train Accuracy      : 0.9791\n",
      "Validation Loss : 0.3849 ,   Valiadtion Accuracy : 0.9132\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "Train Loss      : 0.0604 ,   Train Accuracy      : 0.9863\n",
      "Validation Loss : 0.4278 ,   Valiadtion Accuracy : 0.9105\n",
      "CPU times: user 15min 32s, sys: 8.45 s, total: 15min 41s\n",
      "Wall time: 16min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9133, device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train_model()\n",
    "\n",
    "# As the model's validation loss keeps on increasing after the first epoch,our model is overfitting,\n",
    "# so we will use the model build after the first epoch only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T13:07:16.940297Z",
     "iopub.status.busy": "2021-08-07T13:07:16.939943Z",
     "iopub.status.idle": "2021-08-07T13:07:25.830481Z",
     "shell.execute_reply": "2021-08-07T13:07:25.829455Z",
     "shell.execute_reply.started": "2021-08-07T13:07:16.940251Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentimentModel(\n",
       "  (bert_layer): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout_layer): Dropout(p=0.3, inplace=False)\n",
       "  (linear_layer): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = SentimentModel(3)\n",
    "final_model.load_state_dict(torch.load('../input/trained-model/best_model_0.bin'))\n",
    "final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T13:11:54.488825Z",
     "iopub.status.busy": "2021-08-07T13:11:54.488490Z",
     "iopub.status.idle": "2021-08-07T13:11:54.497316Z",
     "shell.execute_reply": "2021-08-07T13:11:54.496432Z",
     "shell.execute_reply.started": "2021-08-07T13:11:54.488796Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model,data_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    reviews_text = []\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    pred_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for data in data_loader:\n",
    "            \n",
    "            data['x'] = data['x'].cuda()\n",
    "            data['y'] = data['y'].cuda()\n",
    "            data['attention_mask'] = data['attention_mask'].cuda()\n",
    "        \n",
    "            model_output = model(input_ids = data['x'] , attention_mask = data['attention_mask'])\n",
    "\n",
    "            _,pred = torch.max(model_output , dim=1) # This will return the max value and the position(0,1 or 2).\n",
    "            \n",
    "            pred_proba = nn.functional.softmax(model_output , dim=1) # Because model output is just after a linear layer.We want to compress it btw 0 and 1.\n",
    "            \n",
    "            predictions.extend(pred)\n",
    "            actuals.extend(data['y'])\n",
    "            reviews_text.extend(data['review_text'])\n",
    "            pred_probabilities.extend(pred_proba)\n",
    "        \n",
    "        # Converting in tensors using stack method and putting into CPU\n",
    "        return reviews_text,torch.stack(predictions).cpu(),torch.stack(actuals).cpu(),torch.stack(pred_probabilities).cpu()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T13:11:54.852593Z",
     "iopub.status.busy": "2021-08-07T13:11:54.852251Z",
     "iopub.status.idle": "2021-08-07T13:12:08.979900Z",
     "shell.execute_reply": "2021-08-07T13:12:08.978979Z",
     "shell.execute_reply.started": "2021-08-07T13:11:54.852565Z"
    }
   },
   "outputs": [],
   "source": [
    "reviews,predicted,actuals,pred_proba = predict(model,test_DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T13:22:06.853179Z",
     "iopub.status.busy": "2021-08-07T13:22:06.852707Z",
     "iopub.status.idle": "2021-08-07T13:22:06.897798Z",
     "shell.execute_reply": "2021-08-07T13:22:06.894288Z",
     "shell.execute_reply.started": "2021-08-07T13:22:06.853137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy               : 0.923\n",
      "\n",
      "Matthews_Corr_coef     : 0.882\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90      1273\n",
      "           1       0.99      0.96      0.97      1074\n",
      "           2       0.90      0.93      0.91      1643\n",
      "\n",
      "    accuracy                           0.92      3990\n",
      "   macro avg       0.93      0.92      0.93      3990\n",
      "weighted avg       0.92      0.92      0.92      3990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(actuals,predicted)\n",
    "mathews_coeff = matthews_corrcoef(actuals,predicted)\n",
    "confusion = confusion_matrix(actuals,predicted)\n",
    "class_report = classification_report(actuals,predicted)\n",
    "\n",
    "print(f\"Accuracy               : {accuracy:.3f}\")\n",
    "print(f\"\\nMatthews_Corr_coef     : {mathews_coeff:.3f}\")\n",
    "print(f'\\n{class_report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-07T13:25:27.420344Z",
     "iopub.status.busy": "2021-08-07T13:25:27.419945Z",
     "iopub.status.idle": "2021-08-07T13:25:27.738517Z",
     "shell.execute_reply": "2021-08-07T13:25:27.737707Z",
     "shell.execute_reply.started": "2021-08-07T13:25:27.420297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  plt.ylabel('True Sentiment')\n",
    "  plt.xlabel('\\nPredicted Sentiment');\n",
    "\n",
    "df_cm = pd.DataFrame(confusion, index=[\"Negative\",\"Neutral\",\"Positive\"], columns=[\"Negative\",\"Neutral\",\"Positive\"])\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
